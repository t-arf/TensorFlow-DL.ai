Week 2 Wrap up
This week you looked at taking your tokenized words and using Embeddings to establish meaning from them in a mathematical way.
Words were mapped to vectors in higher dimensional space, and the semantics of the words then learned when those words were labelled
with similar meaning. So, for example, when looking at movie reviews, those movies with positive sentiment had the dimensionality of
their words ending up ‘pointing’ a particular way, and those with negative sentiment pointing in a different direction.
From this, the words in future sentences could have their ‘direction’ established, and from this the sentiment inferred.
You then looked at sub word tokenization, and saw that not only do the meanings of the words matter, but also the sequence in which they
are found.

